{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOgVU3l/BcABhfPNn2ok8d8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Run Book to Train CodeLlama model on custom data..**"],"metadata":{"id":"Dn7xkbk0PSxj"}},{"cell_type":"markdown","source":["Step 1 - Letâ€™s **import** all the required libs.. such as PyTorch, trl, transformers, and datasets. We will need these components to train the model on the dataset. **bold text**"],"metadata":{"id":"dVmyNN_ZEF3u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1EufYjWLAgQE"},"outputs":[],"source":["# Install Pytorch & other libraries\n","!pip install \"torch==2.1.2\" tensorboard\n","\n","# Install Hugging Face libraries\n","!pip install  --upgrade \\\n","  \"transformers==4.36.2\" \\\n","  \"datasets==2.16.1\" \\\n","  \"accelerate==0.26.1\" \\\n","  \"evaluate==0.4.1\" \\\n","  \"bitsandbytes==0.42.0\" \\\n","  # \"trl==0.7.10\" # \\\n","  # \"peft==0.7.1\" \\\n","\n","# install peft & trl from github\n","!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n","!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade"]},{"cell_type":"markdown","source":["Once this is done, please install flash-attn. This method can accelerate training up to three times."],"metadata":{"id":"fUP2dHsaEC1g"}},{"cell_type":"code","source":["import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n","# install flash-attn\n","!pip install ninja packaging\n","!MAX_JOBS=4 pip install flash-attn --no-build-isolation"],"metadata":{"id":"XdaXofUnES1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Connect with Hugging face\n"],"metadata":{"id":"Segpm64wFXYp"}},{"cell_type":"code","source":["from huggingface_hub import login\n","\n","login(\n","  token=\"\", # ADD YOUR TOKEN HERE\n","  add_to_git_credential=True\n",")"],"metadata":{"id":"yOVaTh2PFZyX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets prepare the data"],"metadata":{"id":"5RJ3aWIpIIsY"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Convert dataset to OAI messages\n","system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n","SCHEMA:\n","{schema}\"\"\"\n","\n","def create_conversation(sample):\n","  return {\n","    \"messages\": [\n","      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n","      {\"role\": \"user\", \"content\": sample[\"question\"]},\n","      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n","    ]\n","  }\n","\n","# Load dataset from the hub\n","dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n","dataset = dataset.shuffle().select(range(12500))\n","\n","# Convert dataset to OAI messages\n","dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n","# split dataset into 10,000 training samples and 2,500 test samples\n","dataset = dataset.train_test_split(test_size=2500/12500)\n","\n","print(dataset[\"train\"][345][\"messages\"])\n","\n","# save datasets to disk\n","dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n","dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"],"metadata":{"id":"aypgTr6FIKtz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the dataset"],"metadata":{"id":"QBp2EePLJcKr"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load jsonl data from disk\n","dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")"],"metadata":{"id":"Z9BnBWeTJeHA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fine Tuning Using TRL and SFTTrainer.\n"],"metadata":{"id":"vJXBXMBpKXnm"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from trl import setup_chat_format\n","\n","# Hugging Face model id\n","model_id = \"codellama/CodeLlama-7b-hf\" # or `mistralai/Mistral-7B-v0.1`\n","\n","# BitsAndBytesConfig int-4 config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","# Load model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    attn_implementation=\"flash_attention_2\",\n","    torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.padding_side = 'right' # to prevent warnings\n","\n","# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n","model, tokenizer = setup_chat_format(model, tokenizer)"],"metadata":{"id":"HvLis4FIKcV_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["QLora config"],"metadata":{"id":"lkV6eF7-LPv7"}},{"cell_type":"code","source":["from peft import LoraConfig\n","\n","# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n","peft_config = LoraConfig(\n","        lora_alpha=128,\n","        lora_dropout=0.05,\n","        r=256,\n","        bias=\"none\",\n","        target_modules=\"all-linear\",\n","        task_type=\"CAUSAL_LM\",\n",")"],"metadata":{"id":"SeLcWN_4LNlh","executionInfo":{"status":"ok","timestamp":1723425696743,"user_tz":240,"elapsed":363,"user":{"displayName":"Amit Arora","userId":"04997972343776842027"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Setting up Training Parameters.\n"],"metadata":{"id":"_61yfQx5Lb4L"}},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","args = TrainingArguments(\n","    output_dir=\"code-llama-7b-text-to-sql\", # directory to save and repository id\n","    num_train_epochs=3,                     # number of training epochs\n","    per_device_train_batch_size=3,          # batch size per device during training\n","    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n","    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n","    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n","    logging_steps=10,                       # log every 10 steps\n","    save_strategy=\"epoch\",                  # save checkpoint every epoch\n","    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n","    bf16=True,                              # use bfloat16 precision\n","    tf32=True,                              # use tf32 precision\n","    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n","    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n","    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n","    push_to_hub=True,                       # push model to hub\n","    report_to=\"tensorboard\",                # report metrics to tensorboard\n",")"],"metadata":{"id":"8Hl43HxqLheN","executionInfo":{"status":"ok","timestamp":1723425765975,"user_tz":240,"elapsed":369,"user":{"displayName":"Amit Arora","userId":"04997972343776842027"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Create an SFTTrainer"],"metadata":{"id":"EQZjtz1PL3L1"}},{"cell_type":"code","source":["from trl import SFTTrainer\n","\n","max_seq_length = 3072 # max sequence length for model and packing of the dataset\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    dataset_kwargs={\n","        \"add_special_tokens\": False,  # We template with special tokens\n","        \"append_concat_token\": False, # No need to add additional separator token\n","    }\n",")"],"metadata":{"id":"VkjVizarL5dB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train the Model"],"metadata":{"id":"glDlaN8KMJOi"}},{"cell_type":"code","source":["# start training, the model will be automatically saved to the hub and the output directory\n","trainer.train()\n","\n","# save model\n","trainer.save_model()"],"metadata":{"id":"JAxlJ2YvML8Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test the model - Setup"],"metadata":{"id":"Onme_W1VM6uh"}},{"cell_type":"code","source":["import torch\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, pipeline\n","\n","peft_model_id = \"./code-llama-7b-text-to-sql\"\n","# peft_model_id = args.output_dir\n","\n","# Load Model with PEFT adapter\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","  peft_model_id,\n","  device_map=\"auto\",\n","  torch_dtype=torch.float16\n",")\n","tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n","# load into pipeline\n","pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"],"metadata":{"id":"AXORxxDhM9FB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the test dataset and test"],"metadata":{"id":"lae96Oc7NGYG"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from random import randint\n","\n","\n","# Load our test dataset\n","eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n","rand_idx = randint(0, len(eval_dataset))\n","\n","# Test on sample\n","prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n","outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","print(f\"Prompt:\\n{prompt}\")\n","print(f\"Prompt:\\n{eval_dataset[rand_idx]}\")\n","print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n","print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n","print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"],"metadata":{"id":"QCLz8SBkNIey"},"execution_count":null,"outputs":[]}]}